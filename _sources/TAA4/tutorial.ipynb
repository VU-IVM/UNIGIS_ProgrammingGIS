{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5sp-k7_J4No",
        "tags": []
      },
      "source": [
        "# TAA4: Unsupervised learning\n",
        "\n",
        "In this tutorial we will work with 4-digit zipcode-level data from Dutch neighbourhoods to cluster them by similarity. In this tutorial you will learn about the nuances of data pre-processing for unsupervised learning, running and evaluating KMeans, and finally about interpreting the results of unsupervised learning.\n",
        "\n",
        "### Important before we start\n",
        "---\n",
        "This is a 1-week exercise rather than the 2-week exercises of the previous weeks. For that reason, it contains more pre-filled code. You will have the chance to experiment a little bit for some questions for those that find the coding too easy, though.\n",
        "\n",
        "**❗Questions are indicated in full bold face.** They should be filled in on Canvas, and we suggest explaining your rationale and thoughts, as they are intended to test your understanding of the ML pipeline.\n",
        "\n",
        "**⚠️ Make sure that you copy this file** before you continue, else you will lose everything. To do so, go to Bestand/File and click on Een kopie opslaan in Drive/Save a Copy on Drive!\n",
        "\n",
        "**⚠️ Partial use of AI is allowed.** You may use it to generate code, and to check your understanding of key concepts. HOWEVER, don’t use it to generate full answers to questions. We want to see questions written in your own words!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rZlfJGIJ4Nq",
        "tags": []
      },
      "source": [
        "## Learning Objectives\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-0N_aN9J4Nq"
      },
      "source": [
        "* Querying data from a Web Feature Service (WFS)  \n",
        "* Understanding the nuances of data processing for unsupervised learning\n",
        "* Understanding metrics for unsupervised learning, as well as their downsides\n",
        "* Learning to post-hoc interpret cluster centroids\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZeUKHdjJ4Nq"
      },
      "source": [
        "## Packages\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRYN73z7J4Nr"
      },
      "source": [
        "The following packages are noteworthy in this tutorial:\n",
        "\n",
        "[**Pandas**](https://pandas.pydata.org/docs/) is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
        "\n",
        "[**GeoPandas**](https://geopandas.org/) is a Python package that extends the datatypes used by pandas to allow spatial operations on geometric types.\n",
        "\n",
        "[**Matplotlib**](https://matplotlib.org/) is a comprehensive Python package for creating static, animated, and interactive visualizations in Python. Matplotlib makes easy things easy and hard things possible.\n",
        "\n",
        "[**contextily**](https://contextily.readthedocs.io/en/latest/) Contextily provides one-line basemap loading for plotting in Python, which makes it much easier to add context to your visualized spatial data.\n",
        "\n",
        "[**owslib**](https://owslib.readthedocs.io/en/latest/) owslib is a package that wraps requests to spatial API endpoints, such as web feature services, making it easier to load spatial data from APIs.\n",
        "\n",
        "[**scikit-learn**](https://scikit-learn.org/stable/) is a Python package for statistical learning that is built on the general NumPy ecosystem. It is the most-used package for classical machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGSEG_rE_DyC"
      },
      "outputs": [],
      "source": [
        "!pip install contextily -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGlcSIpglsu_"
      },
      "outputs": [],
      "source": [
        "# Built-ins\n",
        "import requests\n",
        "\n",
        "# Data wrangling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import contextily as ctx\n",
        "import seaborn as sns\n",
        "\n",
        "# Analysis\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ima4uWLYWFz"
      },
      "source": [
        "# Dataset preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec7eYXnTT-Lr"
      },
      "source": [
        "In this tutorial we will load a dataset straight from an API, which as you will see comes with its own set of necessary pre-processing steps. We will first download the dataset, followed by some clean-up and lastly some pre-processing steps to prepare the data for clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBGaU7fZYi0o"
      },
      "source": [
        "## Load data for the Randstad area\n",
        "In order to find patterns in unstructured data, we of course first need data. For our example we will use neighbourhood-level statistics provided by the Central Burea for Statistics (CBS) Netherlands. The bounding box below is set to cover the Randstad area and Noord-Holland at the 4-digit zipcode level. The script below will query the Web Feature Service (WFS) and write a local copy so that we can then load it and analyze it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dMkSWWS8kXg"
      },
      "outputs": [],
      "source": [
        "# WFS endpoint for CBS Postcode4 2023\n",
        "url = \"https://service.pdok.nl/cbs/postcode4/2023/wfs/v1_0\"\n",
        "\n",
        "# Bounding box in EPSG:28992\n",
        "bbox = (76000, 430000, 162000, 550000)\n",
        "\n",
        "params = dict(\n",
        "    service=\"WFS\",\n",
        "    version=\"1.0.0\",\n",
        "    request=\"GetFeature\",\n",
        "    typeName=\"postcode4:postcode4\",\n",
        "    srsName=\"EPSG:28992\",\n",
        "    bbox=\",\".join(map(str, bbox))    # Add bounding box filter\n",
        ")\n",
        "\n",
        "# Fetch raw response\n",
        "response = requests.get(url, params=params)\n",
        "\n",
        "# Save to GML\n",
        "with open(\"zipcode4_data.gml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Lbart9y560"
      },
      "source": [
        "With the data written out to the virtual machine, we can load it and have a look at the table structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LvfdW7My44x"
      },
      "outputs": [],
      "source": [
        "# Load into GeoPandas\n",
        "gdf = gpd.read_file(\"zipcode4_data.gml\")\n",
        "gdf.set_crs(epsg=28992, inplace=True)\n",
        "print(f\"Total number of rows: {len(gdf)}\")\n",
        "gdf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wCQTsn5Lsz8"
      },
      "source": [
        "As you can tell (maybe with the help of a translator), the dataset is comprised of a mix of demographic data, building data, distance data, and descriptive metadata about the zipcode area."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6xdbytiPS9f"
      },
      "source": [
        "## Handle missing data\n",
        "This dataset is a decent representation of what you can expect working with real-world data. It has no-data columns, as well as rows which had their values omitted for privacy reasons (e.g. in a small neighbourhood, certain demographic stats are hidden). Working with KMeans requires us to make very deliberate choices on how we handle these, as otherwise they might skew the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1C7sBWQLUhd"
      },
      "source": [
        "Let's first have a look at the scale of the problem. The dataset marks no-data values as any values below zero (as everything is count or demographic data). Let's first look at the columns with non-included data (code -99995). Write some code to find these columns. If you do it right, you should have 98 columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUTTNu_tLRNp"
      },
      "outputs": [],
      "source": [
        "# Find a way to create a list with columns that are invalid (code -99995)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wU0bKTVMRNT"
      },
      "source": [
        "That's about 2/3rds of the dataset! This data is present at a different spatial scale, but not at the 4-digit zipcode level. So, let's just remove these columns from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QlMueG9l8ho"
      },
      "outputs": [],
      "source": [
        "# Drop the columns you identify from the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Tpdzl_NAVN"
      },
      "source": [
        "Next, we will look at omitted data, with the error code -99997. Identify which columns have rows with the value -99997.<br>\n",
        "⚠️ BUT don't drop these columns from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dWhYIqBNLwO"
      },
      "outputs": [],
      "source": [
        "# Repeat the process for error code -99997"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyNuCb9TN9t2"
      },
      "source": [
        "So there are quite a few columns with data that has been omitted. But how widespread is the problem? How frequently do they appear per row? If it's just a single no-data value every now and then, it's not a big issue. But what about when it's more widespread?\n",
        "\n",
        "Let's look at statistics for no-data values per row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qyIE3uaP5l4"
      },
      "outputs": [],
      "source": [
        "# Check rows by iterating and checking values\n",
        "rows_with_missing_count = 0\n",
        "total_missing_values = 0\n",
        "\n",
        "for _, row in gdf_cleaned.iterrows():\n",
        "    missing_in_row = (row == -99997).sum()\n",
        "    if missing_in_row > 0:\n",
        "        rows_with_missing_count += 1\n",
        "        total_missing_values += missing_in_row\n",
        "\n",
        "print(\"Total rows with missing data:\", rows_with_missing_count)\n",
        "\n",
        "# Calculate average number of missing columns per affected row\n",
        "if rows_with_missing_count > 0:\n",
        "    avg_missing_per_row = total_missing_values / rows_with_missing_count\n",
        "    print(\"Average number of missing columns per affected row:\", round(avg_missing_per_row, 2))\n",
        "else:\n",
        "    print(\"No rows with missing data found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7vik8FRQNFH"
      },
      "source": [
        "So, about half of the rows in the dataframe have missing data, and on average 5 rows are affected by this. That's quite substantial, and will require a careful approach. Recall that The K-Means algorithm optimizes based on distance metrics. That is, it iterates to minimize the distance between points belonging to a certain cluster, and to maximize the distance to points outside of it. So, if we want to still use these variables, we need a numerical solution to the no-data problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGl7F6bCQ8GE"
      },
      "source": [
        "Fortunately, we have a very simple heuristic to work with. We know that this omission happens for privacy reasons in small zipcode regions. In other words, we can generally assume that omitted data has very small quantities. We can therefore apply a straightforward stop-gap solution, namely to simply replace these no-data values with zeroes.\n",
        "\n",
        "Building on this, we have a decision to make on whether or not we want to remove rows with excessive no-data values. Are no-data values informative or not? This is up to you as a modeller to decide. On the one hand, by our heuristic, no data-values *may* be informative, as they inform the model of the scale of the neighbourhood. On the other hand, is the size of the neighbourhood informative in itself in the clustering problem? That's up to you to decide. In the default case we're including them, but we've left some code for you to experiment with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXwbAt9WNQXD"
      },
      "outputs": [],
      "source": [
        "# # Exclude rows with more than 3 columns containing -99997\n",
        "# no_data_mask = (gdf_cleaned == -99997).sum(axis=1) <= 3\n",
        "# gdf_filtered = gdf_cleaned[no_data_mask].copy()\n",
        "gdf_filtered = gdf_cleaned\n",
        "\n",
        "# Replace -99997 with 0 (assumed very small quantity due to small neighbourhood size)\n",
        "numeric_cols = gdf_filtered.select_dtypes(include=[np.number]).columns.tolist()\n",
        "for col in numeric_cols:\n",
        "    gdf_filtered[col] = gdf_filtered[col].replace(-99997, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF4HIwOoNSPt"
      },
      "source": [
        "## Visualizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G20OFAobX-Sw"
      },
      "source": [
        "Now that we've handled erroneous data from the dataframe, let's take a moment to visualize our data. You can change the column name to visualize other columns. `gdf_filtered.keys()` gives an overview of available columns (only in Dutch, though translators can help)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhgNIe6YFQNh"
      },
      "outputs": [],
      "source": [
        "# Column to visualize\n",
        "col_name = \"gemiddeldeWozWaardeWoning\" # Housing prices estimated by the govt.\n",
        "\n",
        "# Check if the column exists\n",
        "if col_name in gdf_filtered.columns:\n",
        "    # Convert to Web Mercator for basemap compatibility\n",
        "    gdf_filtered_3857 = gdf_filtered.to_crs(epsg=3857) # Use gpd function `.to_crs()`\n",
        "\n",
        "    # Create side-by-side plots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "    # Choropleth map\n",
        "    gdf_filtered_3857.plot(\n",
        "        column=col_name,\n",
        "        cmap='viridis',\n",
        "        legend=True,\n",
        "        ax=ax1,\n",
        "        alpha=0.8,\n",
        "        edgecolor='k',\n",
        "        linewidth=0.5\n",
        "    )\n",
        "    ctx.add_basemap(ax1, crs=gdf_filtered_3857.crs, source=ctx.providers.OpenStreetMap.Mapnik)\n",
        "    ax1.set_title(f\"Spatial distribution of {col_name}\")\n",
        "    ax1.axis('off')\n",
        "\n",
        "    # Histogram\n",
        "    gdf_filtered[col_name].hist(bins=30, ax=ax2)\n",
        "    ax2.set_title(f'Distribution of {col_name}')\n",
        "    ax2.set_xlabel(col_name)\n",
        "    ax2.set_ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"Column '{col_name}' not found in the GeoDataFrame.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JLr6ocPbT2j"
      },
      "source": [
        "Notice in the distribution that we have some unintended side-effects, such as zeroes in the housing price. More sophisticated methods can probably solve this, but it's a bit beyond the scope of this short tutorial. Let your imagination run wild!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLiVJVVeQ4Gu"
      },
      "source": [
        "❗**Q1: In the case presented in this tutorial, everything gets assigned to zeroes because we have prior knowledge on the structure of the underlying data. In cases where data is missing and we don't have prior knowledge, which other solutions can you come up with to impute these values?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzlwuk1WI_Gj"
      },
      "source": [
        "## Fix non-ratio'd columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYPEYxEQam8s"
      },
      "source": [
        "Now that we have fixed our dataset, we can have a look at the actual structure of the columns. Let's visualize a couple of rows again to inspect what kinds of values are found in each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMLej2fSbyu8"
      },
      "outputs": [],
      "source": [
        "gdf_filtered.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znixaAeib2Zg"
      },
      "source": [
        "*\"Inwoner\"* translates to \"inhabitant\". Notice that we have columns with counts. If you think about how KMeans predicts, you'll understand that these counts are not informative, because zipcode areas are not uniform in size. Instead, we should convert everything to ratios, as these can be compared between areas. Fortunately, we have total counts of everything that we should ratio. The code provided below will do this for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRvVKwqWI9nK"
      },
      "outputs": [],
      "source": [
        "def make_ratios(gdf_to_ratio):\n",
        "    gdf_ratio = gdf_to_ratio.copy() # Otherwise we overwrite the original gdf\n",
        "    cols = gdf_ratio.columns\n",
        "\n",
        "    # 1. Population ratios\n",
        "    pop_cols = [col for col in cols if col in ['aantalMannen', 'aantalVrouwen'] or col.startswith('aantalInwoners')]\n",
        "    for col in pop_cols:\n",
        "        if col != 'aantalInwoners':\n",
        "            gdf_ratio[col] = np.where(gdf_ratio['aantalInwoners'] != 0, gdf_ratio[col] / gdf_ratio['aantalInwoners'], 0)\n",
        "\n",
        "    # 2. Household ratios\n",
        "    hh_cols = [col for col in cols if 'huishoudens' in col and col != 'gemiddeldeHuishoudensgrootte' and col != 'aantalPartHuishoudens']\n",
        "    for col in hh_cols:\n",
        "        gdf_ratio[col] = np.where(gdf_ratio['aantalPartHuishoudens'] != 0, gdf_ratio[col] / gdf_ratio['aantalPartHuishoudens'], 0)\n",
        "\n",
        "    # 3. Housing ratios\n",
        "    woning_cols = [col for col in cols if col.startswith('aantalWoningen') and col != 'aantalWoningen']\n",
        "    for col in woning_cols:\n",
        "        gdf_ratio[col] = np.where(gdf_ratio['aantalWoningen'] != 0, gdf_ratio[col] / gdf_ratio['aantalWoningen'], 0)\n",
        "\n",
        "    return gdf_ratio\n",
        "\n",
        "gdf_ratiod = make_ratios(gdf_filtered)\n",
        "gdf_ratiod.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOuV7NvIcpYi"
      },
      "source": [
        "❗**Q2: How do you think that the model behave if we don't convert columns like this? Could the model theoretically still learn ratio differences between neighbourhoods? Think hard about how K-Means classification works (distance-based optimization problem), and what transforming into ratios is supposed to solve.**\n",
        "\n",
        "*Simply put, the model would separate zipcode areas by the total size of the population, rather than by the relative differences between population groups. In effect, it would not use the variables in a way that we'd like them to, because we understand that demographic information is supposed to be used to compare neighbourhoods by their composition, rather than their flat counts.*\n",
        "\n",
        "*In this case, transforming to ratios shrinks and transforms variables through divison by a given total. Realize that this process inherently transforms distance. K-Means is entirely based on minimizing and maximizing distance, and it doesn't learn the relationship between variables. As such, if we don't convert sub-categories to ratios, then the only meaningful signal to the model is the size of the municipality size, and not their relative differences. By turning variables into ratios, we move the focus away from the relative size of municipalities, and more so on the relationship between variables. Because K-Means does not encode (i.e. it does not learn, only solve), we need to make this information explicit, otherwise the model will focus simply on the relative inhabitant size difference between zipcode regions, and not take into account demographic distributions.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3SSLdYRgxER"
      },
      "source": [
        "### **Normalizing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChQvWbcUdcJm"
      },
      "source": [
        "As mentioned, KMeans models minimize a distance objective function, where the dsitance is determined by the values of the variables. This means that a variable with values in the range 100 to 1'000 will have a much stronger effect on the model's decision boundary than a variable with values ranging from 0.0 to 1.0. As such, it's good practice to normalize variables, so that they have the same importance to the model. To do this, we can *standardize* each variable (also called z-score normalization):\n",
        "\n",
        "$$\n",
        "z = \\frac{x - \\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( x \\) is the original variable value  \n",
        "- \\( \\mu \\) is the mean of the variable\n",
        "- \\( \\sigma \\) is the standard deviation of the variable\n",
        "\n",
        "Let's apply it to each column that we intend to include in the clustering problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk3kwjb2gvV3"
      },
      "outputs": [],
      "source": [
        "numeric_cols = gdf_ratiod.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols = [col for col in numeric_cols if not col in [\"postcode\"]]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "gdf_norm = gdf_ratiod.copy()\n",
        "gdf_norm[numeric_cols] = scaler.fit_transform(gdf_norm[numeric_cols])\n",
        "\n",
        "gdf_norm.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qp9FpefgQsE"
      },
      "source": [
        "Now, all of the values above are z-score normalized, which brings them all to the same range while still preserving meaningful differences compared to the mean of each variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKOcTDTY-NPs"
      },
      "source": [
        "❗**Q3: z-score normalization isn't the only way to normalize data. We can also normalize to a 0-1 range by min/max normalizing: `(value - min) / (max - min)`. What are the benefits and downsides of using this method of normalizing, especially compared to z-score normalization? When would it be suitable to use compared to z-score normalization?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_F3P5rQ65jS"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axhckK2tV5Sb"
      },
      "source": [
        "## Running and evaluating K-Means clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma8AGgJfg8qD"
      },
      "source": [
        "Now that we've prepared the data, we can move towards clustering it. A constant problem with unsupervised learning is deciding which model is the 'right' model. Numerical quantities have been invented for this, which mostly describe the quality of the optimization outcome based on the intended function of KMeans (maximize distance to out-of-cluster points, minimize distance between points within same cluster). Let's quickly discuss these three metrics in order to understand what they're supposed to signal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKzfrvuyh9rF"
      },
      "source": [
        "**Inertia**\n",
        "\n",
        "$$\n",
        "\\text{Inertia} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\| x - \\mu_i \\|^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( k \\) is the number of clusters  \n",
        "- \\( C_i \\) is the set of points in cluster \\( i \\)  \n",
        "- \\( \\mu_i \\) is the centroid of cluster \\( i \\)\n",
        "\n",
        "Inertia measures the sum of squared distances between each point and the centroid of its assigned cluster. In effect, it tells you how tightly-packed clusters are. Understand though that adding more clusters *always* reduces inertia, so it's not just a matter of picking the lowest inertia. Usually these plots are eyeballed for the most obvious break-point with the \"elbow method\", where you look at where the inertia loss from adding new clusters has diminishing returns. Yes, this is pretty subjective, but that's a recurring theme in unsupervised learning. It's a data exploration tool first and foremost.\n",
        "\n",
        "![0_aY163H0kOrBO46S-.webp](data:image/webp;base64,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)\n",
        "<br>\n",
        "([Zala Rushirajsinh on Medium, 2023](https://medium.com/@zalarushirajsinh07/the-elbow-method-finding-the-optimal-number-of-clusters-d297f5aeb189))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJEqVmILj1k0"
      },
      "source": [
        "**Calinski-Harabasz Index**\n",
        "\n",
        "$$\n",
        "\\text{CHI} = \\frac{\\text{Tr}(B_k)}{\\text{Tr}(W_k)} \\cdot \\frac{n - k}{k - 1}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( \\text{Tr}(B_k) \\) is the trace of the between-cluster dispersion matrix  \n",
        "- \\( \\text{Tr}(W_k) \\) is the trace of the within-cluster dispersion matrix  \n",
        "- \\( n \\) is the number of samples  \n",
        "- \\( k \\) is the number of clusters\n",
        "\n",
        "The Calinski-Harabasz Index measures the ratio of between-cluster dispersion to within-cluster dispersion, adjusted for the number of clusters and data points. More simply put, it compares how far apart clusters are, compared to how compact they are.\n",
        "\n",
        "Higher ratio values indicate tighter, more spread-out clusters, which means a better numerical solution. Here, we can look for diminishing returns, similar to the elbow method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B40-3udwlqPL"
      },
      "source": [
        "**Davies-Bouldin Index**\n",
        "$$\n",
        "\\text{DBI} = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\ne i} \\left( \\frac{\\sigma_i + \\sigma_j}{d_{ij}} \\right)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( \\sigma_i \\) is the average distance of all points in cluster \\( i \\) to the centroid of cluster \\( i \\)  \n",
        "- \\( d_{ij} \\) is the distance between the centroids of clusters \\( i \\) and \\( j \\)  \n",
        "- \\( k \\) is the number of clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfkVYjaDl5zF"
      },
      "source": [
        "The Davies-Bouldin index measures the average similarity between each cluster and its most similar one, where similarity is a ratio of within-cluster scatter to between-cluster separation. Simply put once again, it looks at how similarly-dispersed clusters are. Lower values are better, as the objective is to create minimal overlap between all clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHVL9xd8pEuw"
      },
      "source": [
        "Now let's plot these three metrics and see what we get..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDt1jK_t_vU-"
      },
      "outputs": [],
      "source": [
        "def plot_kmeans_metrics(X, cluster_range=range(2, 11)):\n",
        "    \"\"\"\n",
        "    Plots KMeans clustering metrics (inertia, Calinski-Harabasz, Davies-Bouldin)\n",
        "    side-by-side for a given dataset and cluster range.\n",
        "\n",
        "    Parameters:\n",
        "    - X: ndarray or DataFrame, shape (n_samples, n_features)\n",
        "    - cluster_range: iterable of integers, number of clusters to evaluate\n",
        "    - filename: str, name of the output image file\n",
        "    \"\"\"\n",
        "    inertias = []\n",
        "    ch_scores = []\n",
        "    db_scores = []\n",
        "\n",
        "    for k in cluster_range:\n",
        "        kmeans = KMeans(n_clusters=k, n_init=10, random_state=0)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        inertias.append(kmeans.inertia_)\n",
        "        ch_scores.append(calinski_harabasz_score(X, labels))\n",
        "        db_scores.append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
        "\n",
        "    axes[0].plot(cluster_range, inertias, marker='o')\n",
        "    axes[0].set_title('Inertia (lower=better)')\n",
        "    axes[0].set_xlabel('Number of clusters')\n",
        "    axes[0].set_ylabel('Value')\n",
        "\n",
        "    axes[1].plot(cluster_range, ch_scores, marker='o', color='green')\n",
        "    axes[1].set_title('Calinski-Harabasz Index (higher=better)')\n",
        "    axes[1].set_xlabel('Number of clusters')\n",
        "    axes[1].set_ylabel('Value')\n",
        "\n",
        "    axes[2].plot(cluster_range, db_scores, marker='o', color='red')\n",
        "    axes[2].set_title('Davies-Bouldin Index (lower=better)')\n",
        "    axes[2].set_xlabel('Number of clusters')\n",
        "    axes[2].set_ylabel('Value')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE7hAyRS_3uM"
      },
      "outputs": [],
      "source": [
        "X = gdf_norm[numeric_cols].values\n",
        "plot_kmeans_metrics(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HYnbdkHpKTo"
      },
      "source": [
        "But wait, that's strange? Why are these metric plots so strange? For inertia, there is no clear elbow. For CHI, cluster overlap seems to increase as we add clusters. DBI behaves erratically, with no clear best choice either (maybe 9..?)\n",
        "\n",
        "Let's have a look at the clusters that it actually produces to try to make sense of it. Let's use k=4, since the \"elbow\" seems most promising here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACQKjvRtpsJ4"
      },
      "outputs": [],
      "source": [
        "k = 4\n",
        "kmeans = KMeans(n_clusters=k, n_init=10, random_state=0)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "gdf_norm['cluster_label'] = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE3jvcJvptAU"
      },
      "outputs": [],
      "source": [
        "# Convert to Web Mercator for contextily\n",
        "gdf_norm_3857 = gdf_norm.to_crs(epsg=3857)\n",
        "\n",
        "# Plot choropleth\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "gdf_norm_3857.plot(\n",
        "    column='cluster_label',\n",
        "    cmap='viridis',\n",
        "    legend=True,\n",
        "    ax=ax,\n",
        "    alpha=0.5,\n",
        "    edgecolor='k',\n",
        "    linewidth=0.5\n",
        ")\n",
        "ctx.add_basemap(ax, crs=gdf_norm_3857.crs, source=ctx.providers.OpenStreetMap.Mapnik)\n",
        "ax.set_title(\"Neighbourhood clusters\")\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XzQfXthqzzd"
      },
      "source": [
        "Let's have a look at the clusters that it actually produces to try to make sense of it. Let's use k=4, since the \"elbow\" seems most promising here.\n",
        "\n",
        "So, by our cluster metrics, the results shouldn't necessarily be better with more clusters. However, the computed clusters seem to have clear and logical patterns, namely the degree of urbanisation within each postcode:\n",
        "1. Areas with the Groene Hart (\"green heart\", between Rotterdam and Utrecht) almost all belong to the same cluster. **This cluster is shared with the more rural and less populated zipcodes**.\n",
        "2. Another *cluster captures most of the suburban* and less densely populated urban towns and cities.\n",
        "3. Thirdly, **there is a clear urbanism cluster** which covers more densely-populated areas, such as Leiden, Delft, Amsterdam, Utrecht, and Rotterdam.\n",
        "4. **The final cluster seems to be correlated with industry**. In the south, Rotterdam Botlek shows up as part of this cluster, and so does Amsterdam Westpoort near Amsterdam. Both of these are shipping areas. This might have been informed by the lack of population data in this area, as these zipcodes are mostly buildings without many inhabitants, and without housing prices (as these are given for residential buildings only).\n",
        "\n",
        "As can be seen, unsupervised learning is essentially a way to extract key take-aways from the input variables, and a-priori you often don't know what you're going to get out of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYQbawdctB80"
      },
      "source": [
        "❗**Q4: Why does the CHI metric decrease instead of increase in this context, when seemingly logical clusters can be formed? (HINT: Think about how cities tend to form a gradient, in combination with what the CHI metric is supposed to be measuring).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tFRix2POoc0"
      },
      "source": [
        "Feel free to check how the clusters change with different values of K - see if you can make sense of them still! The next section will continue with `k=4`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCPrZGepwgzk"
      },
      "source": [
        "# Analysis of clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8QvQzIO3qpv"
      },
      "source": [
        "Now that we have our clusters and a spatial understanding of what they mean, we can also attempt to understand them numerically. First, let's make a dataframe with just the variables so that we can compute summary statistics over them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txGNZGqEwpCo"
      },
      "outputs": [],
      "source": [
        "# Copy and clean the GeoDataFrame\n",
        "gdf_analysis = gdf_norm.copy()\n",
        "gdf_analysis = gdf_analysis.drop(['geometry', 'postcode', 'gml_id', 'fuuid'], axis=1, errors='ignore')\n",
        "\n",
        "# Separate features and cluster labels\n",
        "features = gdf_analysis.drop(columns='cluster_label')\n",
        "labels = gdf_analysis['cluster_label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "depQf6316-kt"
      },
      "source": [
        "With cluster-level summary statistics in place, we can have a look at the means and the trends of each variable. But wait, we have 40+ variables, how are we going to make sense of this?\n",
        "\n",
        "We can use tools from supervised learning to help out here. Specifically, we can treat it as a reconstruction problem, where the label is the dependent variable, and the variables of the dataframe are independent variables used to predict it. By doing so, we can understand which of the 40+ variables are most important for splitting the space into distinct clusters. You learned how to do this last week, so let's do this here too. We're going to fit and train on the entire dataset at once, so do the following:\n",
        "1. instantiate a `RandomForestClassifier` using the given number of trees\n",
        "2. using `features` and `labels`, train the model\n",
        "3. predict values on the training features using `rf.predict(features)`\n",
        "3. Calculate and print the accuracy of the model using `accuracy_score`. Simply evaluate it on `labels` and the predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn1d1vCy3ph4"
      },
      "outputs": [],
      "source": [
        "# Instantiate a `RandomForestClassifier` using `features` and `labels`\n",
        "# Then, use `accuracy\n",
        "n_trees = 100\n",
        "rf = RandomForestClassifier(..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSFZyEgx-A36"
      },
      "source": [
        "❗**Q5: What does this perfect accuracy score mean? And why do we purposely not split the dataset into train, val, and test sets?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8Fmcatr7oP9"
      },
      "source": [
        "Now that we have reconstructed the clusters derived with unsupervised methods, we can apply a tool you learned about last week: *feature importance*. Recall that feature importance in the context of RF models is how likely a split in a tree with this variable is to partition an example into different classes. Can you see how this translates to separability between clusters in this case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOrRWw9K8yX4"
      },
      "source": [
        "❗**Q6: In your own words, explain how Random Forest feature importance helps to explain which variables have the greatest influence on forming clusters.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5nXOjmh-Yck"
      },
      "source": [
        "Now that we've fitted our model, we can have a look at the most important features for splitting the clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLbb9yqc9Xe5"
      },
      "outputs": [],
      "source": [
        "# Get feature importances from the Random Forest model\n",
        "# Be sure to sort by the importance. It can be found in the `rf` variable\n",
        "# Remember what you learned in last week's TAA!\n",
        "importance =\n",
        "\n",
        "# Plot feature importances\n",
        "# Assumed to be a Pandas dataframe with column importance in order\n",
        "# Change the code or the approach if you use a different data structure\n",
        "x_feats = important_features.values\n",
        "y_feats = important_features.index\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=x_feats, y=y_feats, palette='viridis')\n",
        "plt.title('Top 10 Feature Importances')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzZSSTwu_Kgn"
      },
      "source": [
        "You can translate these most important variables with the help of Google Translate, simply print and copy-paste the entire output:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WHuTChj_aUD"
      },
      "outputs": [],
      "source": [
        "print([k for k in important_features.keys()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OEWnuO8_4TE"
      },
      "source": [
        "To better understand each variable, we can also look at their distribution within each cluster, to understand if a variable correlates positively, neutrally, or negatively with each cluster. Have a look at the summary plots below, and try to interpret them. Remember that these are z-score normalized variables. A positive score means an above average large quantity of the , whereas a negative z-score means that there is very little of it present.\n",
        "\n",
        "Try to plot or print aggregation statistics for each cluster (mean, std, min, max) for the most important variables. HINT: Pandas has `.groupby` and `.agg` functions that would be useful for this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCrCVeF0_VvZ"
      },
      "outputs": [],
      "source": [
        "# Plot or print descriptive statistics for each important feature\n",
        "# Again assumed to be a Pandas dataframe, but feel free to use your own approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NQVMhU_AzgC"
      },
      "source": [
        "Have a look at some of the means and standard deviations and try to understand how they relate to the clusters.\n",
        "\n",
        "❗**Q7: Which numerical conclusions are you able to derive from the clusters? Are you able to make sense of the clusters? Which trends and patterns emerge for each of these clusters based on the most important variables?**\n",
        "\n",
        "* *cluster 0 shows clear demographic trends towards modern urban dwellers. Younger, more international, more one-person households, and more densely urban.*\n",
        "\n",
        "* *Similarly, cluster 1 shows trends for suburban and peripheral living, such as more families, fewer multi-family houses, with more Dutch nationals.*\n",
        "\n",
        "* *In that sense, these results corroborate the visual analysis of the maps, although not all variables are clear and easy to explain, and it should be clear that there is still a lot of manual interpretation and implicit narrative-forming involved, with potentially contrasting narratives able to be found in the data.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzTFtqTFBMHw"
      },
      "source": [
        "# Wrapping up\n",
        "That concludes our general introduction to the workflow of exploration using unsupervised learning. Hopefully you now understand why unsupervised methods are quite strongly up to interpretation, but also how they can lead to surprising results and emerging patterns. You've learned to prepare data for unsupervised classification, some of the pre-processing decisions that you have to make before exploring, and finally the process of exploration using KMeans Clustering. You've learned about the metrics that you can use to analyze this type of unsupervised learning problem, but also learned how these metrics don't always tell the full story of what the data is trying to express. Lastly, you've learned some methods for the post-hoc analysis of clusters, and how you can try to make sense of the main patterns for each cluster. Ultimately, unsupervised learning is an exploration tool that is only as good as its user, and ultimately it still requires human interpretation to understand what each cluster represents.\n",
        "\n",
        "So go out there and cluster, but remember to be sure to explain your decisions in the clustering process!\n",
        "\n",
        "**Bonus!**<br>\n",
        "While coming up with this tutorial, we also used Principal Component Analysis (PCA) to try to reduce the dimensionality of the data, to see if patterns would change. Ultimately this did not matter, but we decided to keep it in to show you how PCA can be used to reduce the number of variables, which makes the prediction problem easier in case you're dealing with the curse of dimensionality (more variables than datapoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J74eSLj4AQwb"
      },
      "source": [
        "## Using PCA to reduce dimensionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuYY8f1qEe6d"
      },
      "source": [
        "PCA is a method that reduces the dimensionality of data by summarizing the variance (main patterns) across all variables. In that sense, it's similar to KMeans, but its purpose is usually to reduce the number of variables, rather than to be interpreted. Here, we use PCA to reduce the total number of variables that will be used in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APiI7FcQXCYI"
      },
      "outputs": [],
      "source": [
        "# Assume gdf_norm is your normalized DataFrame (numeric columns only, no geometry)\n",
        "X = gdf_norm[numeric_cols].values  # or the columns you want to use\n",
        "\n",
        "# Fit PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.title('PCA: Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XzMHxJeEWzF"
      },
      "source": [
        "Based on the above plot, you pick the number of components. Usually you pick a number of components that explain ~90% of the variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rKiKTHvazeY"
      },
      "outputs": [],
      "source": [
        "n_components = 12\n",
        "\n",
        "# Reduce data to n_components\n",
        "pca = PCA(n_components=n_components)\n",
        "X_reduced = pca.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4Xh9B45BGnp"
      },
      "source": [
        "### Re-run and re-interpret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkMVDncNBLCz"
      },
      "outputs": [],
      "source": [
        "plot_kmeans_metrics(X_reduced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "somaGC_KEzr0"
      },
      "source": [
        "The patterns in the plots haven't changed in a meaningful manner, which indicates that PCA doesn't do much to generate better separable clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6iB6mH1T53A"
      },
      "outputs": [],
      "source": [
        "k = 4\n",
        "kmeans = KMeans(n_clusters=k, n_init=10, random_state=0)\n",
        "labels = kmeans.fit_predict(X_reduced)\n",
        "\n",
        "gdf_norm['cluster_label'] = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGQJ1HSwUnAT"
      },
      "outputs": [],
      "source": [
        "# Convert to Web Mercator for contextily\n",
        "gdf_norm_3857 = gdf_norm.to_crs(epsg=3857)\n",
        "\n",
        "# Plot choropleth\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "gdf_norm_3857.plot(\n",
        "    column='cluster_label',\n",
        "    cmap='viridis',\n",
        "    legend=True,\n",
        "    ax=ax,\n",
        "    alpha=0.8,\n",
        "    edgecolor='k',\n",
        "    linewidth=0.5\n",
        ")\n",
        "ctx.add_basemap(ax, crs=gdf_norm_3857.crs, source=ctx.providers.OpenStreetMap.Mapnik)\n",
        "ax.set_title(\"Neighbourhood clusters\")\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeuEDm-TE-37"
      },
      "source": [
        "Here we can see that the main patterns are still the same, and the PCA hasn't resulted in any meaningful shifts. However, if we would have a problem with many more variables than rows, PCA could be used to generate more meaningful clusters."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "J74eSLj4AQwb"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
